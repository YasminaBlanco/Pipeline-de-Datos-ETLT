# Airflow

Este m贸dulo contiene la configuraci贸n del orquestador Apache Airflow, responsable de programar, ejecutar y monitorear los flujos de trabajo ETLT (DAGs).

##  Contenido

* `dags/`: Contiene los archivos DAG de Python.
    * [spark_etl_dag.py](dags/spark_etl_dag.py): El flujo de trabajo principal que orquesta la ejecuci贸n de los scripts de Spark ETL (capa Silver y Gold) a trav茅s de un operador SSH o similar.
* `.env`: Variables de entorno espec铆ficas para Airflow (usuario/contrase帽a de la UI).
* `docker-compose.yaml` (Ra铆z): Define el servicio `airflow-webserver` y `airflow-scheduler`.

## 锔 Flujo de Trabajo Principal (`spark_etl_dag.py`)

Este DAG es el responsable de lanzar los trabajos pesados de transformaci贸n de datos de una capa a otra.

1.  **Verificar** la conectividad con los servicios Spark.
2.  **Ejecutar el ETL Silver:** Lanza el script `capa_silver.py` en el contenedor de Spark.
3.  **Ejecutar el ETL Gold:** Lanza el script `capa_gold.py` en el contenedor de Spark (depende del 茅xito del Silver).

### Dependencias y Conexiones

Este DAG requiere una conexi贸n llamada `spark-connection` (t铆picamente de tipo SSH si el motor Spark est谩 en otro contenedor o servidor) para enviar comandos de ejecuci贸n a la m谩quina donde reside el c贸digo Spark.

### Uso en Producci贸n

Tras iniciar Docker Compose, accede a la UI de Airflow (`http://<IP_PBLICA>:8080`), busca el DAG `spark_etl_final` y act铆valo. Se ejecutar谩 autom谩ticamente en el horario programado o puede ser lanzado manualmente.

